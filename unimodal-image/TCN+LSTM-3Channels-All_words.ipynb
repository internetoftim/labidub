{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-9d076fdf7b21>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'F:/Project/Processed_Data/Mouth_Images_Random/Addresses_Random.txt'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'load_data' is not defined"
     ]
    }
   ],
   "source": [
    "x,y = load_data('F:/Project/Processed_Data/Mouth_Images_Random/Addresses_Random.txt',3,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path,num_word,times):\n",
    "    labels = []\n",
    "    inputs = []\n",
    "    with open(path,'r') as f:\n",
    "        lines = f.readlines()\n",
    "        count = 0\n",
    "        label_count = 0\n",
    "        target_lines = lines[times*25*num_word:times*25*num_word+num_word*25]\n",
    "        for i in target_lines:           \n",
    "            #print(i)\n",
    "            if(count>=(num_word*25)):\n",
    "                break\n",
    "            #temp = Image.open(i.strip('\\n')).convert('L')\n",
    "            temp = Image.open(i.strip('\\n'))\n",
    "            #image_t = np.array(temp)\n",
    "            image_t = np.array(temp)\n",
    "            inputs.append(image_t)\n",
    "            temp.close()\n",
    "            \n",
    "            if((label_count == 0)|(label_count == 25)):\n",
    "                label_count = 0\n",
    "                all_string = i.split('/')\n",
    "                #print(all_string[-1])\n",
    "                #print(all_string)\n",
    "                labels.append((all_string[-1]).split('_')[-2])\n",
    "                #print(labels[-1])\n",
    "            label_count += 1\n",
    "            count += 1\n",
    "            #print(all_string[-1])\n",
    "    #print(labels)\n",
    "    x = np.asarray(inputs, dtype = np.float32)\n",
    "    #x = x/255.0\n",
    "    return x,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 80, 100, 3)\n"
     ]
    }
   ],
   "source": [
    "x,y = load_data('F:/Project/Process_Small_Python/bin_please/train.txt',2,1)\n",
    "print(x.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(label):\n",
    "    \n",
    "    word_map = ['bin','lay','place','set',\n",
    "               'blue','red','green','white',\n",
    "               'at','in','with','by',\n",
    "               'a','b','c','d','e','f','g','h','i','j','k','l','m','n','o','p','q','r','s','t','u','v','x','y','z',\n",
    "               'zero','one','two','three','four','five','six','seven','eight','nine',\n",
    "               'again','now','soon','please']\n",
    "    word_to_int = dict((c,i) for i,c in enumerate(word_map))\n",
    "    #print(word_to_int)\n",
    "    int_to_word = dict((i,c) for i,c in enumerate(word_map))\n",
    "    #print(target[1])\n",
    "    #print(word_to_int[target[1]])\n",
    "    interger_encoded = [word_to_int[word] for word in label]\n",
    "    \n",
    "    onehot_encoded = list()\n",
    "    for value in interger_encoded:\n",
    "        letter = [0 for _ in range(len(word_map))]\n",
    "        letter[value] = 1\n",
    "        onehot_encoded.append(letter)\n",
    "    result = np.asarray(onehot_encoded,dtype = np.int32)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape(x,y):\n",
    "    return(x.reshape(-1,25,80,100,3),y.reshape(-1,51))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(path,steps_per_epoch,epochs):\n",
    "    #for ii in range(2):\n",
    "    for ii in range(epochs):\n",
    "        for i in range(int(114000/steps_per_epoch)):\n",
    "        \n",
    "            x,y = load_data(path,steps_per_epoch,i)\n",
    "            output = one_hot(y)\n",
    "            inputs,label = reshape(x,output)\n",
    "            yield(inputs,label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, LSTM, TimeDistributed,Convolution2D,MaxPooling2D,Activation,Flatten,BatchNormalization,Dropout\n",
    "from keras.models import Sequential\n",
    "from keras import optimizers\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.layers.core import Reshape\n",
    "from keras import optimizers\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import keras.backend as K\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "time_distributed_1 (TimeDist (None, 25, 80, 100, 96)   2688      \n",
      "_________________________________________________________________\n",
      "time_distributed_2 (TimeDist (None, 25, 26, 33, 96)    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_3 (TimeDist (None, 25, 26, 33, 256)   221440    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 25, 26, 33, 256)   1024      \n",
      "_________________________________________________________________\n",
      "time_distributed_4 (TimeDist (None, 25, 8, 11, 256)    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_5 (TimeDist (None, 25, 8, 11, 512)    1180160   \n",
      "_________________________________________________________________\n",
      "time_distributed_6 (TimeDist (None, 25, 8, 11, 512)    2359808   \n",
      "_________________________________________________________________\n",
      "time_distributed_7 (TimeDist (None, 25, 8, 11, 512)    2359808   \n",
      "_________________________________________________________________\n",
      "time_distributed_8 (TimeDist (None, 25, 2, 3, 512)     0         \n",
      "_________________________________________________________________\n",
      "time_distributed_9 (TimeDist (None, 25, 3072)          0         \n",
      "_________________________________________________________________\n",
      "lstm_layer (LSTM)            (None, 25, 512)           7342080   \n",
      "_________________________________________________________________\n",
      "time_distr_dense_one (TimeDi (None, 25, 256)           131328    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 512)               1574912   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2048)              1050624   \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 51)                104499    \n",
      "=================================================================\n",
      "Total params: 16,328,371\n",
      "Trainable params: 16,327,859\n",
      "Non-trainable params: 512\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(TimeDistributed(Convolution2D(96, (3, 3),padding='same'),\n",
    "                          input_shape=(25,80,100,3)))\n",
    "model.add(TimeDistributed(MaxPooling2D(pool_size=(3, 3))))\n",
    "\n",
    "model.add(TimeDistributed(Convolution2D(256, (3, 3),padding='same')))\n",
    "model.add(BatchNormalization())\n",
    "#model.add(Activation('relu'))\n",
    "model.add(TimeDistributed(MaxPooling2D(pool_size=(3, 3))))\n",
    "#model.add(Dropout(0.2))\n",
    "\n",
    "model.add(TimeDistributed(Convolution2D(512, (3, 3),padding='same')))\n",
    "#model.add(BatchNormalization())\n",
    "model.add(TimeDistributed(Convolution2D(512, (3, 3),padding='same')))\n",
    "#model.add(BatchNormalization())\n",
    "model.add(TimeDistributed(Convolution2D(512, (3, 3),padding='same')))\n",
    "#model.add(BatchNormalization())\n",
    "model.add(TimeDistributed(MaxPooling2D(pool_size=(3, 3))))\n",
    "\n",
    "model.add(TimeDistributed(Flatten()))\n",
    "model.add(LSTM(512, return_sequences=True, name=\"lstm_layer\"))\n",
    "model.add(TimeDistributed(Dense(256,activation='softmax'), name=\"time_distr_dense_one\"))\n",
    "\n",
    "model.add(LSTM(512, return_sequences=False, stateful=False))\n",
    "#model.add(TimeDistributed(Dense(256,activation='softmax'), name=\"time_distr_dense_one2\"))\n",
    "model.add(Dense(2048))\n",
    "model.add(((Dense(51,activation='softmax'))))\n",
    "Adam = optimizers.Adamax(lr=0.05, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='Adam',metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = Sequential()\n",
    "model.load_weights('first_two38.h5')\n",
    "#K.set_value(Adam.lr, (1-((1e-6)*14)) * K.get_value(Adam.lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator_test(path,num_word):\n",
    "    for ii in range(1):\n",
    "        for i in range(6000):\n",
    "            x,y = load_data(path,num_word,i)\n",
    "            output = one_hot(y)\n",
    "            #print(\"111\")0\n",
    "            inputs,label = reshape(x,output)\n",
    "            yield (inputs,label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "5700/5700 [==============================] - 7820s 1s/step - loss: 3.1398 - acc: 0.1281\n",
      "[2.9678128027915953, 0.13250000272567072]\n",
      "Epoch 1/1\n",
      "5700/5700 [==============================] - 8574s 2s/step - loss: 3.0709 - acc: 0.1424\n",
      "[2.984708185195923, 0.13191666934639215]\n",
      "Epoch 1/1\n",
      "5700/5700 [==============================] - 8199s 1s/step - loss: 3.0590 - acc: 0.1456\n",
      "[2.976596350669861, 0.13116666942213973]\n",
      "Epoch 1/1\n",
      "5700/5700 [==============================] - 7809s 1s/step - loss: 3.0576 - acc: 0.1467\n",
      "[2.964789400100708, 0.12683333606148758]\n",
      "Epoch 1/1\n",
      "5700/5700 [==============================] - 7624s 1s/step - loss: 3.0555 - acc: 0.1476\n",
      "[2.9602037886778514, 0.12775000264247258]\n",
      "Epoch 1/1\n",
      "5700/5700 [==============================] - 7609s 1s/step - loss: 3.0525 - acc: 0.1479\n",
      "[2.941417522033056, 0.13075000268096726]\n",
      "Epoch 1/1\n",
      "5700/5700 [==============================] - 7565s 1s/step - loss: 3.0406 - acc: 0.1529\n",
      "[2.9518502163887024, 0.13325000268096726]\n",
      "Epoch 1/1\n",
      "5700/5700 [==============================] - 7589s 1s/step - loss: 3.0431 - acc: 0.1499\n",
      "[2.944537159204483, 0.13100000269090137]\n",
      "Epoch 1/1\n",
      "5700/5700 [==============================] - 7564s 1s/step - loss: 3.0262 - acc: 0.1549\n",
      "[2.8871931719779966, 0.16783333652342358]\n",
      "Epoch 1/1\n",
      "5700/5700 [==============================] - 7554s 1s/step - loss: 3.0097 - acc: 0.1576\n",
      "[2.8676922404766083, 0.17291666991387805]\n",
      "Epoch 1/1\n",
      "5700/5700 [==============================] - 7508s 1s/step - loss: 3.0227 - acc: 0.1533\n",
      "[2.928071341117223, 0.14158333618814747]\n",
      "Epoch 1/1\n",
      "5700/5700 [==============================] - 7602s 1s/step - loss: 3.0000 - acc: 0.1596\n",
      "[2.920432585080465, 0.1571666696233054]\n",
      "Epoch 1/1\n",
      "5700/5700 [==============================] - 7560s 1s/step - loss: 3.0572 - acc: 0.1483\n",
      "[2.9689778319994606, 0.13033333602671823]\n",
      "Epoch 1/1\n",
      "5700/5700 [==============================] - 7630s 1s/step - loss: 3.0458 - acc: 0.1492\n",
      "[2.947068914572398, 0.13725000282128652]\n",
      "Epoch 1/1\n",
      "5700/5700 [==============================] - 7866s 1s/step - loss: 2.8151 - acc: 0.2056\n",
      "[2.5004772458473843, 0.2766666689949731]\n",
      "Epoch 1/1\n",
      "5700/5700 [==============================] - 7994s 1s/step - loss: 3.0547 - acc: 0.1479\n",
      "[2.850048828125, 0.16466666971022884]\n",
      "Epoch 1/1\n",
      "5700/5700 [==============================] - 7781s 1s/step - loss: 3.0276 - acc: 0.1534\n",
      "[2.8393302551905313, 0.17833333633840084]\n",
      "Epoch 1/1\n",
      "5700/5700 [==============================] - 7866s 1s/step - loss: 2.7948 - acc: 0.2118\n",
      "[2.461012349128723, 0.2791666695599755]\n",
      "Epoch 1/1\n",
      "5700/5700 [==============================] - 7766s 1s/step - loss: 2.5601 - acc: 0.2687\n",
      "[2.828393589258194, 0.1800000034024318]\n",
      "Epoch 1/1\n",
      "5700/5700 [==============================] - 7738s 1s/step - loss: 2.5321 - acc: 0.2793\n",
      "[2.440923298001289, 0.3058333356430133]\n",
      "Epoch 1/1\n",
      "5700/5700 [==============================] - 7678s 1s/step - loss: 2.3500 - acc: 0.3244\n",
      "[2.304433223605156, 0.3272500017471611]\n",
      "Epoch 1/1\n",
      "5700/5700 [==============================] - 7637s 1s/step - loss: 2.3809 - acc: 0.3159\n",
      "[2.2656835587819417, 0.3573333352431655]\n",
      "Epoch 1/1\n",
      "5700/5700 [==============================] - 7566s 1s/step - loss: 2.1236 - acc: 0.3801\n",
      "[2.0904356547196707, 0.3690833345428109]\n",
      "Epoch 1/1\n",
      "5700/5700 [==============================] - 7579s 1s/step - loss: 2.0345 - acc: 0.4055\n",
      "[2.8093451005220413, 0.26675000282004474]\n",
      "Epoch 1/1\n",
      "5700/5700 [==============================] - 8151s 1s/step - loss: 1.9974 - acc: 0.4127\n",
      "[1.9638963359594346, 0.39666666796430944]\n",
      "Epoch 1/1\n",
      "5700/5700 [==============================] - 8000s 1s/step - loss: 1.9602 - acc: 0.4246\n",
      "[2.0865199949343998, 0.39975000073512396]\n",
      "Epoch 1/1\n",
      "5700/5700 [==============================] - 7804s 1s/step - loss: 2.2262 - acc: 0.3554\n",
      "[2.1984663236141206, 0.32908333517611027]\n",
      "Epoch 1/1\n",
      "5700/5700 [==============================] - 7788s 1s/step - loss: 2.0888 - acc: 0.3867\n",
      "[2.1797099727392197, 0.3575000013783574]\n",
      "Epoch 1/1\n",
      "5700/5700 [==============================] - 7726s 1s/step - loss: 2.1056 - acc: 0.3780\n",
      "[2.440161092678706, 0.29358333588888247]\n",
      "Epoch 1/1\n",
      "5700/5700 [==============================] - 7645s 1s/step - loss: 2.0312 - acc: 0.4033\n",
      "[3.7572869857152305, 0.17291666962206365]\n",
      "Epoch 1/1\n",
      "5700/5700 [==============================] - 7600s 1s/step - loss: 1.9441 - acc: 0.4248\n",
      "[1.6263761533300083, 0.49508333479364713]\n",
      "Epoch 1/1\n",
      "5700/5700 [==============================] - 7601s 1s/step - loss: 1.8627 - acc: 0.4468\n",
      "[1.7517233473062515, 0.46450000184277696]\n",
      "Epoch 1/1\n",
      "5700/5700 [==============================] - 7520s 1s/step - loss: 1.6746 - acc: 0.4972\n",
      "[1.7023037698864938, 0.4811666677022974]\n",
      "Epoch 1/1\n",
      "5700/5700 [==============================] - 7590s 1s/step - loss: 1.7580 - acc: 0.4762\n",
      "[1.7455297382672628, 0.47008333476881187]\n",
      "Epoch 1/1\n",
      "5700/5700 [==============================] - 7490s 1s/step - loss: 1.6758 - acc: 0.4976\n",
      "[1.6967970677713553, 0.49900000167389713]\n",
      "Epoch 1/1\n",
      "5700/5700 [==============================] - 7610s 1s/step - loss: 1.5478 - acc: 0.5330\n",
      "[1.7361899936695893, 0.5021666684746742]\n",
      "Epoch 1/1\n",
      "5700/5700 [==============================] - 7495s 1s/step - loss: 1.4650 - acc: 0.5560\n",
      "[1.777260882059733, 0.5008333343515794]\n",
      "Epoch 1/1\n",
      "5700/5700 [==============================] - 7541s 1s/step - loss: 1.4008 - acc: 0.5736\n",
      "[1.8325487449765205, 0.48916666804502407]\n",
      "Epoch 1/1\n",
      "5700/5700 [==============================] - 7476s 1s/step - loss: 1.3476 - acc: 0.5899\n",
      "[1.7682081518570583, 0.5042500010753671]\n",
      "Epoch 1/1\n",
      "5700/5700 [==============================] - 7538s 1s/step - loss: 1.3034 - acc: 0.6020\n",
      "[1.8852909851074218, 0.4898333345601956]\n"
     ]
    }
   ],
   "source": [
    "num_word = 20\n",
    "epochs = 40\n",
    "for epoch in range(epochs):\n",
    "    history = model.fit_generator(data_generator('F:/Project/Process_Small_Python/all_words_random/train.txt',num_word,epochs), verbose=1, workers=2,\n",
    "                              steps_per_epoch=int(114000/num_word), epochs = 1)\n",
    "    time = epoch\n",
    "    model.save_weights('first_two'+str(time)+'.h5')\n",
    "    score = model.evaluate_generator(data_generator_test('F:/Project/Process_Small_Python/all_words_random/test.txt'\n",
    "                                                    ,20),steps=600)\n",
    "    print(score)\n",
    "    #K.set_value(Adam.lr, (1-(1e-4)) * K.get_value(Adam.lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12000, 51)\n"
     ]
    }
   ],
   "source": [
    "score = model.predict_generator(data_generator_test('F:/Project/Process_Small_Python/all_words_random/test.txt'\n",
    "                                                    ,20),steps=600)\n",
    "print(score.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot():\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['acc']\n",
    "    plt.plot(loss)\n",
    "    plt.plot(val_loss)\n",
    "    plt.legend(['loss', 'acc'])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11111\n",
      "[2.7000083893934885, 0.21283333333333335]\n"
     ]
    }
   ],
   "source": [
    "print(\"11111\")\n",
    "#output = one_hot(y)\n",
    "#x_test,y_test = reshape(x,output,25)\n",
    "score = model.evaluate_generator(data_generator_test('F:/Project/Process_Small_Python/all_words_test/test.txt'\n",
    "                                                    ,2),steps=3000)\n",
    "\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_y(path):\n",
    "    labels = []\n",
    "    #inputs = []\n",
    "    with open(path,'r') as f:\n",
    "        lines = f.readlines()\n",
    "        print(len(lines))\n",
    "        count = 0\n",
    "        label_count = 0\n",
    "        #target_lines = lines\n",
    "        for i in lines:           \n",
    "            #print(i)\n",
    "            #if(count>=(num_word*25)):\n",
    "                #break\n",
    "            #temp = Image.open(i.strip('\\n')).convert('L')\n",
    "            #temp = Image.open(i.strip('\\n'))\n",
    "            #image_t = np.array(temp)\n",
    "            #image_t = np.array(temp)\n",
    "            #inputs.append(image_t)\n",
    "            #temp.close()\n",
    "            \n",
    "            if((label_count == 0)|(label_count == 25)):\n",
    "                label_count = 0\n",
    "                all_string = i.split('/')\n",
    "                #print(all_string[-1])\n",
    "                #print(all_string)\n",
    "                labels.append((all_string[-1]).split('_')[-2])\n",
    "                #print(labels[-1])\n",
    "            label_count += 1\n",
    "            count += 1\n",
    "            #print(all_string[-1])\n",
    "    #print(labels)\n",
    "    #x = np.asarray(inputs, dtype = np.float32)\n",
    "    #x = x/255.0\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300000\n",
      "12000\n"
     ]
    }
   ],
   "source": [
    "only_y = load_y('F:/Project/Process_Small_Python/all_words_random/test.txt')\n",
    "print(len(y))\n",
    "hot_y = one_hot(only_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(label):\n",
    "    \n",
    "    word_map = ['bin','lay','place','set',\n",
    "               'blue','red','green','white',\n",
    "               'at','in','with','by',\n",
    "               'a','b','c','d','e','f','g','h','i','j','k','l','m','n','o','p','q','r','s','t','u','v','x','y','z',\n",
    "               'zero','one','two','three','four','five','six','seven','eight','nine',\n",
    "               'again','now','soon','please']\n",
    "    word_to_int = dict((c,i) for i,c in enumerate(word_map))\n",
    "    #print(word_to_int)\n",
    "    int_to_word = dict((i,c) for i,c in enumerate(word_map))\n",
    "    #print(target[1])\n",
    "    #print(word_to_int[target[1]])\n",
    "    interger_encoded = [word_to_int[word] for word in label]\n",
    "    \n",
    "    onehot_encoded = list()\n",
    "    for value in interger_encoded:\n",
    "        letter = [0 for _ in range(len(word_map))]\n",
    "        letter[value] = 1\n",
    "        onehot_encoded.append(letter)\n",
    "    result = np.asarray(onehot_encoded,dtype = np.int32)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[0.01955224 0.01958951 0.0196271  0.0195902  0.01956972 0.0196392\n",
      " 0.01957088 0.01965452 0.019557   0.01966373 0.0196252  0.01961722\n",
      " 0.0195812  0.01959906 0.01962689 0.0196409  0.01964178 0.01955035\n",
      " 0.01962658 0.01956737 0.01961608 0.01960778 0.01962405 0.01956657\n",
      " 0.01955927 0.01960347 0.01966549 0.01958907 0.01962623 0.01958452\n",
      " 0.0196307  0.01961634 0.01963692 0.01963503 0.0195704  0.01963333\n",
      " 0.01963364 0.01961233 0.01959748 0.01957119 0.01962866 0.01958621\n",
      " 0.0196631  0.01957931 0.01958702 0.01963642 0.01955977 0.01964775\n",
      " 0.01964326 0.01963185 0.01956612]\n",
      "[  0   0   0   0   0   0   0   0   0  79   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0  34   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0 399   0   0   0   0   0   0   0   0]\n"
     ]
    }
   ],
   "source": [
    "print(hot_y[1])\n",
    "print(score[1])\n",
    "matrix = confusion_matrix(hot_y.argmax(axis=1), score.argmax(axis=1))\n",
    "print(matrix[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_map = ['bin','lay','place','set',\n",
    "               'blue','red','green','white',\n",
    "               'at','in','with','by',\n",
    "               'a','b','c','d','e','f','g','h','i','j','k','l','m','n','o','p','q','r','s','t','u','v','x','y','z',\n",
    "               'zero','one','two','three','four','five','six','seven','eight','nine',\n",
    "               'again','now','soon','please']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        bin       0.00      0.00      0.00       512\n",
      "        lay       0.00      0.00      0.00       512\n",
      "      place       0.00      0.00      0.00       483\n",
      "        set       0.00      0.00      0.00       493\n",
      "       blue       0.00      0.00      0.00       500\n",
      "        red       0.00      0.00      0.00       500\n",
      "      green       0.00      0.00      0.00       500\n",
      "      white       0.00      0.00      0.00       500\n",
      "         at       0.00      0.00      0.00       501\n",
      "         in       0.00      0.00      0.00       504\n",
      "       with       0.00      0.00      0.00       499\n",
      "         by       0.00      0.00      0.00       496\n",
      "          a       0.00      0.00      0.00        80\n",
      "          b       0.00      0.00      0.00        80\n",
      "          c       0.00      0.00      0.00        80\n",
      "          d       0.00      0.00      0.00        80\n",
      "          e       0.00      0.00      0.00        80\n",
      "          f       0.00      0.00      0.00        80\n",
      "          g       0.00      0.00      0.00        80\n",
      "          h       0.00      0.00      0.00        80\n",
      "          i       0.00      0.00      0.00        80\n",
      "          j       0.00      0.00      0.00        80\n",
      "          k       0.00      0.00      0.00        80\n",
      "          l       0.00      0.00      0.00        80\n",
      "          m       0.00      0.00      0.00        80\n",
      "          n       0.00      0.00      0.00        80\n",
      "          o       0.01      0.10      0.01        80\n",
      "          p       0.00      0.00      0.00        80\n",
      "          q       0.00      0.00      0.00        80\n",
      "          r       0.00      0.00      0.00        80\n",
      "          s       0.00      0.00      0.00        80\n",
      "          t       0.00      0.00      0.00        80\n",
      "          u       0.00      0.00      0.00        80\n",
      "          v       0.00      0.00      0.00        80\n",
      "          x       0.00      0.00      0.00        80\n",
      "          y       0.00      0.00      0.00        80\n",
      "          z       0.00      0.00      0.00        80\n",
      "       zero       0.00      0.00      0.00       200\n",
      "        one       0.00      0.00      0.00       200\n",
      "        two       0.00      0.00      0.00       200\n",
      "      three       0.00      0.00      0.00       200\n",
      "       four       0.00      0.00      0.00       200\n",
      "       five       0.01      0.39      0.02       200\n",
      "        six       0.00      0.00      0.00       200\n",
      "      seven       0.00      0.00      0.00       200\n",
      "      eight       0.00      0.00      0.00       200\n",
      "       nine       0.00      0.00      0.00       200\n",
      "      again       0.00      0.00      0.00       500\n",
      "        now       0.00      0.00      0.00       500\n",
      "       soon       0.00      0.00      0.00       500\n",
      "     please       0.00      0.00      0.00       500\n",
      "\n",
      "avg / total       0.00      0.01      0.00     12000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\Softwares\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(hot_y.argmax(axis=1),score.argmax(axis=1), target_names=word_map))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
