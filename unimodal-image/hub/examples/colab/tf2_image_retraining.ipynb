{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ScitaPqhKtuW"
   },
   "source": [
    "##### Copyright 2019 The TensorFlow Hub Authors.\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jvztxQ6VsK2k"
   },
   "outputs": [],
   "source": [
    "# Copyright 2019 The TensorFlow Hub Authors. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# =============================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oYM61xrTsP5d"
   },
   "source": [
    "# TF Hub for TF2: Image Module Retraining (preview)\n",
    "\n",
    "<table align=\"left\">\n",
    "<td align=\"center\">\n",
    "  <a target=\"_blank\"  href=\"https://colab.research.google.com/github/tensorflow/hub/blob/master/examples/colab/tf2_image_retraining.ipynb\">\n",
    "    <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" /><br>Run in Google Colab\n",
    "  </a>\n",
    "</td>\n",
    "<td align=\"center\">\n",
    "  <a target=\"_blank\"  href=\"https://github.com/tensorflow/hub/blob/master/examples/colab/tf2_image_retraining.ipynb\">\n",
    "    <img width=32px src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" /><br>View source on GitHub</a>\n",
    "</td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L1otmJgmbahf"
   },
   "source": [
    "This Colab demonstrates the use of TF2 SavedModels found on TensorFlow Hub with Keras. It uses a pre-trained image feature vector module for classifying five species of flowers, including fine-tuning of the module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bL54LWCHt5q5"
   },
   "source": [
    "## Set up TensorFlow 2 and other libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "txdrFJrJvgQS"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "  # %tensorflow_version only exists in Colab.\n",
    "  %tensorflow_version 2.x\n",
    "except Exception:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dlauq-4FWGZM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF version: 2.0.0\n",
      "Hub version: 0.6.0\n",
      "GPU is available\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import os\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import pathlib\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "print(\"TF version:\", tf.__version__)\n",
    "print(\"Hub version:\", hub.__version__)\n",
    "print(\"GPU is\", \"available\" if tf.test.is_gpu_available() else \"NOT AVAILABLE\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mmaHHH7Pvmth"
   },
   "source": [
    "## Select the Hub/TF2 module to use\n",
    "\n",
    "Hub modules for TF 1.x won't work here, please use one of the selections provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FlsEcKVeuCnf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using https://tfhub.dev/google/tf2-preview/inception_v3/feature_vector/4 with input size (299, 299)\n"
     ]
    }
   ],
   "source": [
    "module_selection = (\"inception_v3\", 299) #@param [\"(\\\"mobilenet_v2\\\", 224)\", \"(\\\"inception_v3\\\", 299)\"] {type:\"raw\", allow-input: true}\n",
    "handle_base, pixels = module_selection\n",
    "MODULE_HANDLE =\"https://tfhub.dev/google/tf2-preview/{}/feature_vector/4\".format(handle_base)\n",
    "IMAGE_SIZE = (pixels, pixels)\n",
    "print(\"Using {} with input size {}\".format(MODULE_HANDLE, IMAGE_SIZE))\n",
    "\n",
    "BATCH_SIZE = 32 #@param {type:\"integer\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yTY8qzyYv3vl"
   },
   "source": [
    "## Set up the Flowers dataset\n",
    "\n",
    "Inputs are suitably resized for the selected module. Dataset augmentation (i.e., random distortions of an image each time it is read) improves training, esp. when fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://console.cloud.google.com/storage/browser/labidub/Dataset.tar.gz\n",
      "  73728/Unknown - 0s 0us/step"
     ]
    }
   ],
   "source": [
    "data_dir = tf.keras.utils.get_file(\n",
    "    'Dataset',\n",
    "#     'gs://labidub/Dataset.tar.gz',\n",
    "    'https://console.cloud.google.com/storage/browser/labidub/Dataset.tar.gz',\n",
    "    untar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying gs://labidub/Dataset_Fill.tar.gz...\n",
      "| [1 files][  4.2 GiB/  4.2 GiB]   82.4 MiB/s                                   \n",
      "Operation completed over 1 objects/4.2 GiB.                                      \n"
     ]
    }
   ],
   "source": [
    "!gsutil cp gs://labidub/Dataset_Fill.tar.gz .\n",
    "!tar xzf Dataset_Fill.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying gs://labidub/Dataset.tar.gz...\n",
      "- [1 files][  1.5 GiB/  1.5 GiB]  110.7 MiB/s                                   \n",
      "Operation completed over 1 objects/1.5 GiB.                                      \n"
     ]
    }
   ],
   "source": [
    "!gsutil cp gs://labidub/Dataset.tar.gz .\n",
    "!tar xzf Dataset.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jupyter/labidub/unimodal-image/hub/examples/colab'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = pathlib.Path('/home/jupyter/labidub/unimodal-image/flower_photos')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = pathlib.Path('./Dataset_Fill')\n",
    "data_dir=str(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ls  . | sed -e 's/\\(.*\\)/train\\/\\1/g' | xargs mkdir -p \n",
    "# ls  . | sed -e 's/\\(.*\\)/test\\/\\1/g' | xargs mkdir -p "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25196 images belonging to 54 classes.\n"
     ]
    }
   ],
   "source": [
    "datagen_kwargs = dict(rescale=1./255, validation_split=.20)\n",
    "dataflow_kwargs = dict(target_size=IMAGE_SIZE, batch_size=BATCH_SIZE,\n",
    "                   interpolation=\"bilinear\")\n",
    "datagen_kwargs = dict(rescale=1./255, validation_split=.20)\n",
    "dataflow_kwargs = dict(target_size=IMAGE_SIZE, batch_size=BATCH_SIZE,\n",
    "                   interpolation=\"bilinear\")\n",
    "test_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    **datagen_kwargs)\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    data_dir, subset=\"validation\", shuffle=False, **dataflow_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "# os.rename(\"path/to/current/file.foo\", \"path/to/new/destination/for/file.foo\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = test_datagen.flow_from_directory(\n",
    "    data_dir, subset=\"training\", shuffle=False, **dataflow_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Dataset_Fill'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "umB5tswsfTEQ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20148 images belonging to 52 classes.\n",
      "Found 80656 images belonging to 52 classes.\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"Dataset_Fill/train\"\n",
    "datagen_kwargs = dict(rescale=1./255, validation_split=.20)\n",
    "dataflow_kwargs = dict(target_size=IMAGE_SIZE, batch_size=BATCH_SIZE,\n",
    "                   interpolation=\"bilinear\")\n",
    "\n",
    "valid_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    **datagen_kwargs)\n",
    "valid_generator = valid_datagen.flow_from_directory(\n",
    "    data_dir, subset=\"validation\", shuffle=False, **dataflow_kwargs)\n",
    "\n",
    "do_data_augmentation = True #@param {type:\"boolean\"}\n",
    "if do_data_augmentation:\n",
    "    train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "      rotation_range=40,\n",
    "      horizontal_flip=True,\n",
    "      width_shift_range=0.2, height_shift_range=0.2,\n",
    "      shear_range=0.2, zoom_range=0.2,\n",
    "      **datagen_kwargs)\n",
    "else:\n",
    "    train_datagen = valid_datagen\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    data_dir, subset=\"training\", shuffle=True, **dataflow_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FS_gVStowW3G"
   },
   "source": [
    "\n",
    "## Defining the model\n",
    "\n",
    "All it takes is to put a linear classifier on top of the `feature_extractor_layer` with the Hub module.\n",
    "\n",
    "For speed, we start out with a non-trainable `feature_extractor_layer`, but you can also enable fine-tuning for greater accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RaJW3XrPyFiF"
   },
   "outputs": [],
   "source": [
    "do_fine_tuning = True #@param {type:\"boolean\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "50FYNIb1dmJH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model with https://tfhub.dev/google/tf2-preview/inception_v3/feature_vector/4\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "keras_layer (KerasLayer)     multiple                  21802784  \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            multiple                  0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                multiple                  106548    \n",
      "=================================================================\n",
      "Total params: 21,909,332\n",
      "Trainable params: 21,874,900\n",
      "Non-trainable params: 34,432\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "print(\"Building model with\", MODULE_HANDLE)\n",
    "model = tf.keras.Sequential([\n",
    "    hub.KerasLayer(MODULE_HANDLE, trainable=do_fine_tuning),\n",
    "    tf.keras.layers.Dropout(rate=0.2),\n",
    "    tf.keras.layers.Dense(train_generator.num_classes, activation='softmax',\n",
    "                          kernel_regularizer=tf.keras.regularizers.l2(0.0001))\n",
    "])\n",
    "model.build((None,)+IMAGE_SIZE+(3,))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u2e5WupIw2N2"
   },
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9f3yBUvkd_VJ"
   },
   "outputs": [],
   "source": [
    "model.compile(\n",
    "  optimizer=tf.keras.optimizers.SGD(lr=0.005, momentum=0.9), \n",
    "  loss=tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.1),\n",
    "  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w_YKX2Qnfg6x"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "2520/2520 [==============================] - 2198s 872ms/step - loss: 1.4291 - accuracy: 0.8299 - val_loss: 2.6958 - val_accuracy: 0.5004\n",
      "Epoch 2/3\n",
      "1691/2520 [===================>..........] - ETA: 11:24 - loss: 1.3987 - accuracy: 0.8395"
     ]
    }
   ],
   "source": [
    "steps_per_epoch = train_generator.samples // train_generator.batch_size\n",
    "validation_steps = valid_generator.samples // valid_generator.batch_size\n",
    "hist = model.fit_generator(\n",
    "    train_generator,\n",
    "    epochs=3, steps_per_epoch=steps_per_epoch,\n",
    "    validation_data=valid_generator,\n",
    "    validation_steps=validation_steps,verbose=1).history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CYOw0fTO1W4x"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fed9475e198>]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.ylabel(\"Loss (training and validation)\")\n",
    "plt.xlabel(\"Training Steps\")\n",
    "plt.ylim([0,2])\n",
    "plt.plot(hist[\"loss\"])\n",
    "plt.plot(hist[\"val_loss\"])\n",
    "\n",
    "plt.figure()\n",
    "plt.ylabel(\"Accuracy (training and validation)\")\n",
    "plt.xlabel(\"Training Steps\")\n",
    "plt.ylim([0,1])\n",
    "plt.plot(hist[\"accuracy\"])\n",
    "plt.plot(hist[\"val_accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': [0.82985216, 0.84089106, 0.8508385],\n",
       " 'loss': [1.4290987213208197, 1.3962311056261618, 1.3634767802042203],\n",
       " 'val_accuracy': [0.50039744, 0.53502584, 0.5633446],\n",
       " 'val_loss': [2.695763185501857, 2.6466522813790934, 2.444194936695463]}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YCsAsQM1IRvA"
   },
   "source": [
    "Finally, the trained model can be saved for deployment to TF Serving or TF Lite (on mobile) as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LGvTi69oIc2d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1781: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1781: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./saved_lip_fill/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./saved_lip_fill/assets\n"
     ]
    }
   ],
   "source": [
    "saved_model_path = \"./saved_lip_fill\"\n",
    "tf.saved_model.save(model, saved_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 5935/25196 [======>.......................] - ETA: 1:00:28"
     ]
    }
   ],
   "source": [
    "# model.evaluate_generator(generator=valid_generator,\n",
    "# steps=STEP_SIZE_VALID)\n",
    "\n",
    "# STEP_SIZE_TEST=valid_generator.n//valid_generator.batch_size\n",
    "# valid_generator.reset()\n",
    "# pred=model.predict_generator(valid_generator,\n",
    "# steps=STEP_SIZE_TEST,\n",
    "# verbose=1)\n",
    "\n",
    "# predicted_class_indices=np.argmax(pred,axis=1)\n",
    "\n",
    "# labels = (train_generator.class_indices)\n",
    "# labels = dict((v,k) for k,v in labels.items())\n",
    "# predictions = [labels[k] for k in predicted_class_indices]\n",
    "\n",
    "\n",
    "filenames = valid_generator.filenames\n",
    "nb_samples = len(filenames)\n",
    "\n",
    "# # filenames = test_generator.filenames\n",
    "# # nb_samples = len(filenames)\n",
    "\n",
    "predict = model.predict_generator(valid_generator,steps = nb_samples,\n",
    "verbose=1)\n",
    "predicted_class_indices=np.argmax(predict,axis=1)\n",
    "\n",
    "# model.predict_generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20148/20148 [==============================] - 3491s 173ms/step\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "arrays must all be same length",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-a20bb612ee65>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mfilenames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalid_generator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilenames\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m results=pd.DataFrame({\"Filename\":filenames,\n\u001b[0;32m---> 13\u001b[0;31m                       \"Predictions\":predictions})\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"results.csv\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    409\u001b[0m             )\n\u001b[1;32m    410\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 411\u001b[0;31m             \u001b[0mmgr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minit_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    412\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMaskedArray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m             \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmrecords\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmrecords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36minit_dict\u001b[0;34m(data, index, columns, dtype)\u001b[0m\n\u001b[1;32m    255\u001b[0m             \u001b[0marr\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_datetime64tz_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0marr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         ]\n\u001b[0;32m--> 257\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marrays_to_mgr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36marrays_to_mgr\u001b[0;34m(arrays, arr_names, index, columns, dtype)\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0;31m# figure out the index, if necessary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36mextract_index\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    366\u001b[0m             \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_lengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 368\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"arrays must all be same length\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhave_dicts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: arrays must all be same length"
     ]
    }
   ],
   "source": [
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "        test_dir,\n",
    "        target_size=(200, 200),\n",
    "        color_mode=\"rgb\",\n",
    "        shuffle = False,\n",
    "        class_mode='categorical',\n",
    "        batch_size=1)\n",
    "\n",
    "filenames = test_generator.filenames\n",
    "nb_samples = len(filenames)\n",
    "\n",
    "predict = model.predict_generator(test_generator,steps = nb_samples)\n",
    "\n",
    "filenames = valid_generator.filenames\n",
    "nb_samples = len(filenames)\n",
    "predict = model.predict_generator(valid_generator,steps = nb_samples,\n",
    "verbose=1)\n",
    "predicted_class_indices=np.argmax(predict,axis=1)\n",
    "\n",
    "labels = (train_generator.class_indices)\n",
    "labels = dict((v,k) for k,v in labels.items())\n",
    "predictions = [labels[k] for k in predicted_class_indices]\n",
    "\n",
    "filenames=valid_generator.filenames\n",
    "results=pd.DataFrame({\"Filename\":filenames,\n",
    "                      \"Predictions\":predictions})\n",
    "results.to_csv(\"results.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20148"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filenames = valid_generator.filenames\n",
    "nb_samples = len(filenames)\n",
    "nb_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "644364"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QzW4oNRjILaq"
   },
   "source": [
    "## Optional: Deployment to TensorFlow Lite\n",
    "\n",
    "[TensorFlow Lite](https://www.tensorflow.org/lite) lets you deploy TensorFlow models to mobile and IoT devices. The code below shows how to convert the trained model to TF Lite and apply post-training tools from the [TensorFlow Model Optimization Toolkit](https://www.tensorflow.org/model_optimization). Finally, it runs it in the TF Lite Interpreter to examine the resulting quality\n",
    "\n",
    "  * Converting without optimization provides the same results as before (up to roundoff error).\n",
    "  * Converting with optimization without any data quantizes the model weights to 8 bits, but inference still uses floating-point computation for the neural network activations. This reduces model size almost by a factor of 4 and improves CPU latency on mobile devices.\n",
    "  * On top, computation of the neural network activations can be quantized to 8-bit integers as well if a small reference dataset is provided to calibrate the quantization range. On a mobile device, this accelerates inference further and makes it possible to run on accelerators like EdgeTPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Va1Vo92fSyV6"
   },
   "outputs": [],
   "source": [
    "#@title Optimization settings\n",
    "optimize_lite_model = False  #@param {type:\"boolean\"}\n",
    "#@markdown Setting a value greater than zero enables quantization of neural network activations. A few dozen is already a useful amount.\n",
    "num_calibration_examples = 60  #@param {type:\"slider\", min:0, max:1000, step:1}\n",
    "representative_dataset = None\n",
    "if optimize_lite_model and num_calibration_examples:\n",
    "  # Use a bounded number of training examples without labels for calibration.\n",
    "  # TFLiteConverter expects a list of input tensors, each with batch size 1.\n",
    "  representative_dataset = lambda: itertools.islice(\n",
    "      ([image[None, ...]] for batch, _ in train_generator for image in batch),\n",
    "      num_calibration_examples)\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_path)\n",
    "if optimize_lite_model:\n",
    "  converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "  if representative_dataset:  # This is optional, see above.\n",
    "    converter.representative_dataset = representative_dataset\n",
    "lite_model_content = converter.convert()\n",
    "\n",
    "with open(\"/tmp/lite_flowers_model\", \"wb\") as f:\n",
    "  f.write(lite_model_content)\n",
    "print(\"Wrote %sTFLite model of %d bytes.\" %\n",
    "      (\"optimized \" if optimize_lite_model else \"\", len(lite_model_content)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_wqEmD0xIqeG"
   },
   "outputs": [],
   "source": [
    "interpreter = tf.lite.Interpreter(model_content=lite_model_content)\n",
    "# This little helper wraps the TF Lite interpreter as a numpy-to-numpy function.\n",
    "def lite_model(images):\n",
    "  interpreter.allocate_tensors()\n",
    "  interpreter.set_tensor(interpreter.get_input_details()[0]['index'], images)\n",
    "  interpreter.invoke()\n",
    "  return interpreter.get_tensor(interpreter.get_output_details()[0]['index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JMMK-fZrKrk8"
   },
   "outputs": [],
   "source": [
    "#@markdown For rapid experimentation, start with a moderate number of examples.\n",
    "num_eval_examples = 50  #@param {type:\"slider\", min:0, max:700}\n",
    "eval_dataset = ((image, label)  # TFLite expects batch size 1.\n",
    "                for batch in train_generator\n",
    "                for (image, label) in zip(*batch))\n",
    "count = 0\n",
    "count_lite_tf_agree = 0\n",
    "count_lite_correct = 0\n",
    "for image, label in eval_dataset:\n",
    "  probs_lite = lite_model(image[None, ...])[0]\n",
    "  probs_tf = model(image[None, ...]).numpy()[0]\n",
    "  y_lite = np.argmax(probs_lite)\n",
    "  y_tf = np.argmax(probs_tf)\n",
    "  y_true = np.argmax(label)\n",
    "  count +=1\n",
    "  if y_lite == y_tf: count_lite_tf_agree += 1\n",
    "  if y_lite == y_true: count_lite_correct += 1\n",
    "  if count >= num_eval_examples: break\n",
    "print(\"TF Lite model agrees with original model on %d of %d examples (%g%%).\" %\n",
    "      (count_lite_tf_agree, count, 100.0 * count_lite_tf_agree / count))\n",
    "print(\"TF Lite model is accurate on %d of %d examples (%g%%).\" %\n",
    "      (count_lite_correct, count, 100.0 * count_lite_correct / count))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "ScitaPqhKtuW"
   ],
   "last_runtime": {
    "build_target": "",
    "kind": "local"
   },
   "name": "TF Hub for TF2: Image Module Retraining (preview)",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
